%%% Template originaly created by Karol Kozio≈Ç (mail@karol-koziol.net) and modified for ShareLaTeX use

\documentclass[a4paper,11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{xcolor}
 \usepackage{tgtermes}
\usepackage{listings}
\usepackage{minted}
 \usepackage[
 pdftitle={Math Assignment},
 pdfauthor={Joe Doe, Some University},
 colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue,bookmarks=true,
 bookmarksopenlevel=2]{hyperref}
\usepackage{amsmath,amssymb,amsthm,textcomp}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{tikz}

\usepackage{geometry}
\geometry{total={210mm,297mm},
left=25mm,right=25mm,%
bindingoffset=0mm, top=20mm,bottom=20mm}


\linespread{1.3}

\newcommand{\linia}{\rule{\linewidth}{0.5pt}}

% custom theorems if needed
\newtheoremstyle{mytheor}
    {1ex}{1ex}{\normalfont}{0pt}{\scshape}{.}{1ex}
    {{\thmname{#1 }}{\thmnumber{#2}}{\thmnote{ (#3)}}}

\theoremstyle{mytheor}
\newtheorem{defi}{Definition}
\usepackage[ruled, vlined, linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage[parfill]{parskip}
\makeatletter

\setlength\parindent{0pt}
% custom footers and headers
\usepackage{fancyhdr,lastpage}


\newcommand{\myequ}[1]{\begin{align}\begin{split} #1 \end{split}\end{align}}


\begin{document}

\title{CSE 250B: Machine Learning}

\author{Sai Bi}

\date{\today}

\maketitle

\section*{Problem 1}
\subsection*{b}
The clustering result is listed as following:
\begin{itemize}
	\item Class 1: tiger leopard fox wolf bobcat lion 
	\item Class 2: antelope horse moose ox giraffe buffalo deer cow 
	\item Class 3: beaver mole squirrel bat rat weasel raccoon 
	\item Class 4: killer+whale blue+whale humpback+whale seal otter walrus dolphin 
    \item Class 5: sheep zebra giant+panda 
    \item Class 6: dalmatian persian+cat german+shepherd siamese+cat chihuahua collie 
   
    \item Class 7:    spider+monkey gorilla chimpanzee 
    
    \item Class 8: grizzly+bear polar+bear 
    
    \item Class 9: hippopotamus elephant rhinoceros pig 
    
    \item Class 10: skunk hamster rabbit mouse 
\end{itemize}
And the clustering results make sense.

\subsection*{c}
See Figure~\ref{fig:1c}. The hierarchical cluster result is sensible.
\begin{figure}
    \centering{
         \includegraphics[width=0.9\textwidth]{./code/1c.png}  
     }
     \caption{Problem 1-c: dendrogram plot of the hierarchical cluster tree.}
     \label{fig:1c}
\end{figure}

\section*{Problem 2}
\subsection*{a}
It is obvious that the energy function is convex, and it achieves global minimum when its derivative is equal to $\mathbf{0}$. Therefore, let $f = \sum\limits_{x\in C}|| x - \mu||^2$
\begin{align}
    \begin{split}
        f' &= \sum\limits_{x \in C} 2 (\mu - x)
    \end{split}
\end{align}
Let $f' = \mathbf{0}$, we will have $\mu = \frac{\sum\limits_{x \in C} x} {|C|}$.
That is, the optimal $\mu$ is simply the mean of the points $C$.

\subsection*{b}
Suppose we have three points $(-1, 0), (1, 0), (6, 0)$, the optimal center location would be $(1,0)$. But the mean of
points is $(2, 0)$. Therefore the optimal center $\mu$ is not the mean of points in the $L_1$ case.

The optimal center in the $(\mathbb{R}^1, L_1)$ case should be the median of all points.

\section*{Problem 3}
\subsection*{a}
The optimal $k$-means solution is $-9, 0, 9$.

\subsection*{b}
Let the initial centers be $-6, 8 , 10$. In this case the final clustering
centers would be $-6, 8, 10$, which is suboptimal.

\section*{Problem 4}
See Figure~\ref{fig:4} (Enlarge the figure to see details). The embedding
is sensible to me.

To do PCA, let $X$ be the data matrix, where each row is a 
data point. First calculate the covariance matrix $\sum$ of $X$, and then get its eigenvalues $\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_p$ and corresponding 
eigenvectors $u_1, u_2,...,u_p$. Select two eigenvectors that correspond to the two largest
eigenvalues, that is $u_1, u_2$. Then for a data point $x$, it can be projected to 
$\mathbb{R}^2$ in the following way:
\begin{align}
    \begin{split}
    x & \mapsto (u_1 \cdot x, u_2 \cdot x)^T
    \end{split}
\end{align}

\begin{figure}
    \centering{
        \includegraphics[width=0.9\textwidth]{./code/4.png}  
    }
    \caption{Problem 4: data points after PCA.}
    \label{fig:4}
\end{figure}

\section*{Problem 5}

\subsection*{a}
\begin{itemize}
    \item The dimensions of $U$ is $p \times 2$.
     \item The dimensions of $U^T$ is $2 \times p$.
     \item The dimensions of $UU^T$ is $p \times p$.
     \item The dimensions of $u_1 u_1^T$ is $p \times p$.
\end{itemize}

\subsection*{b}
The first and the third projections are the same. And the projection is
$\mathbb{R}^p \mapsto \mathbb{R}^{2}$.

The second and the fourth projections are the same. And the projection is 
$\mathbb{R}^p \mapsto \mathbb{R}^p$.






















\end{document}
