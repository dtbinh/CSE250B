%%% Template originaly created by Karol Kozio≈Ç (mail@karol-koziol.net) and modified for ShareLaTeX use

\documentclass[a4paper,11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{xcolor}
 \usepackage{tgtermes}

 \usepackage[
 pdftitle={Math Assignment},
 pdfauthor={Joe Doe, Some University},
 colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue,bookmarks=true,
 bookmarksopenlevel=2]{hyperref}
\usepackage{amsmath,amssymb,amsthm,textcomp}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{tikz}

\usepackage{geometry}
\geometry{total={210mm,297mm},
left=25mm,right=25mm,%
bindingoffset=0mm, top=20mm,bottom=20mm}


\linespread{1.3}

\newcommand{\linia}{\rule{\linewidth}{0.5pt}}

% custom theorems if needed
\newtheoremstyle{mytheor}
    {1ex}{1ex}{\normalfont}{0pt}{\scshape}{.}{1ex}
    {{\thmname{#1 }}{\thmnumber{#2}}{\thmnote{ (#3)}}}

\theoremstyle{mytheor}
\newtheorem{defi}{Definition}
\usepackage[ruled, vlined, linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage[parfill]{parskip}
% my own titles
\makeatletter
% \renewcommand{\maketitle}{
% \begin{center}
% \vspace{2ex}
% {\huge \textsc{\@title}}
% \vspace{1ex}
% \\
% \linia\\
% \@author \hfill \@date
% \vspace{4ex}
% \end{center}
% }
% \makeatother
%%%
\setlength\parindent{0pt}
% custom footers and headers
\usepackage{fancyhdr,lastpage}
% \pagestyle{fancy}
% \lhead{}
% \chead{}
% \rhead{}
% \lfoot{}
% \cfoot{}
% \rfoot{Page \thepage\ /\ \pageref*{LastPage}}
% \renewcommand{\headrulewidth}{0pt}
% \renewcommand{\footrulewidth}{0pt}
%

%%%----------%%%----------%%%----------%%%----------%%%

\begin{document}

\title{CSE 250B: Machine Learning}

\author{Sai Bi}

\date{\today}

\maketitle

\section*{Problem 1}
\subsection*{a} 
Apply \textit{k}-means algorithm to the training data with $k = M$ clusters, and for each cluster, we select its 
centroid as our prototype and associate the prototype with the class that has highest count in that cluster.

\subsection*{b}
{\LinesNumberedHidden
\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetAlgoNoLine
	
	\caption{Clustering prototype selection algorithm}
	% \SetKwInOut{Input}{Input}
	% \SetKwInOut{Output}{Output}
	\textbf{Input: }{$\mathbf{X}^{N \times d}, \mathbf{Y}^{N \times 1}, M$} \\
	\textbf{Output: }{$\mathbf{XP}^{M \times d}, \mathbf{YP}^{M \times 1}$} \\
	\begin{enumerate}
		\item Apply $k$-means clustering to the input data $\mathbf{X}^{N \times d}$, and get their clustering labels $\mathbf{L}^{N \times 1}$ \\
		\item Calculate the average of data points in each cluster $\Omega_i$: $\mathbf{XP}_i = \frac{1}{|\Omega_i|} \sum\limits_{\mathbf{x} \in \Omega_i}{\mathbf{x}}$ \\
		\item Set $\mathbf{YP}_i$ to the class label with most occurrence in that cluster
	\end{enumerate}
\end{algorithm}
}

\subsection*{c}
\begin{tabular}{|l|c|c|c|c|c|}
	% \hline
	% 	&  $1000$ & $2000$ & $5000$ & $10000$  \\ \hline
	% Uniform sampling &  $11.45 \pm 0.11$ \ & $8.92 \pm 0.08 $ & $6.57 \pm 0.05 $ & $5.29 \pm 0.04$  \\ \hline 
	% Prototype selection & $6.44 \pm 0.06$  & $5.31\pm 0.05$ & 5000 & 10000  \\ \hline 
	\hline
		&  $1000$  & $5000$ & $10000$  \\ \hline
	Uniform sampling &  $11.45 \pm 0.11$  & $6.57 \pm 0.05 $ & $5.29 \pm 0.04$  \\ \hline 
	Prototype selection & $6.44 \pm 0.06$  &  $5.10 \pm 0.04$ & $4.21 \pm 0.02$  \\ \hline 
\end{tabular}

\begin{itemize}
	\item All errors all reported in percentage ($\%$).
	\item All experiments are repeated $30$ times.
	\item All errors are reported as a $0.95$ confidence interval.
	\item For $k$-means, maximum iteration number is set to $100$, and initialization is chosen randomly.
\end{itemize}

\section*{Problem 2}
\subsection*{a}
The Bayes optimal classifier is:
\begin{equation}
h(x) = \left\{ \begin{array}{ll}
	1 & \mbox{if $-0.5 \leq  x \leq 0.5$}\\
	0 & \mbox{if $x < -0.5$ or $x > 0.5$}.\end{array} \right.
\end{equation}

The optimal risk $R^*$ is:
\begin{align}
	\begin{split}
		R^* & = \int_{-1}^{-0.5} 0.2 |x|  \text{d} x + \int_{-0.5}^{0.5} 0.2 |x|  \text{d} x + \int_{0.5}^{1} 0.4 |x|  \text{d} x \\
			& = 0.275
	\end{split}
\end{align}

\subsection*{b}
The optimal decision boundary is $-0.6$ and $0.5$.

The error rate is:
\begin{align}
	\begin{split}
		\text{error rate} &=  \int_{-1}^{-0.6} 0.2 |x|  \text{d}x + \int_{-0.6}^{-0.5} 0.8 |x|  \text{d}x	
			+ \int_{-0.5}^{0.5} 0.2 |x|  \text{d}x + \int_{0.5}^{1} 0.4 |x|  \text{d}x \\
			& = 0.308		
	\end{split}
\end{align}

\subsection*{c}
The optimal classifier is:
\begin{equation}
	h(x) = 0, \forall x \in [-1, 1]
\end{equation}

\subsection*{d}

\begin{equation}
h(x) = \left\{ \begin{array}{ll}
0 & \mbox{if $\eta(x) * c_{10} \leq (1-\eta(x)) * c_{01} $}\\
1 & \mbox{otherwise} \end{array} 
\right.
\end{equation}

\section*{Problem 3}
\subsection*{a}
$\ell_1$ distance is a metric:
\begin{enumerate}
	\item non-negativity: $d(x,y) = ||x-y||_1 = \sum\limits_{i} |x_i - y_i| \geq 0$
	\item identity of indiscernibles: $||x-y||_1 = 0   \Leftrightarrow \sum\limits_{i} |x_i - y_i| = 0 \Leftrightarrow \forall i, x_i = y_i 
	\Leftrightarrow x = y$ 
	\item symmetry: $||x-y||_1 = \sum\limits_{i}|x_i - y_i| = \sum\limits_{i}|y_i - x_i| = ||y-x||_1$
	\item triangle inequality: $||x-z||_1 = \sum\limits_{i}|x_i - y_i| \leq \sum\limits_{i}(|x_i-y_i| + |y_i-z_i|) = ||x - y||_1 + ||y-z||_1 $
\end{enumerate}


\subsection*{b}
$d_1 + d_2$ is a metric, let $d = d_1 + d_2$:
\begin{enumerate}
	\item non-negativity: $d(x,y) = d_1(x,y) + d_2(x,y) \geq 0$
	\item identity of indiscernibles: $d(x,y) = 0 \Leftrightarrow d_1(x,y) = 0, d_2(x,y) = 0 \Leftrightarrow x = y$
	\item symmetry: $d(x,y) = d_1(x,y) + d_2(x,y) = d_1(y,x) + d_2(y,x) = d(y,x)$
	\item triangle inequality: $d(x,z) = d_1(x,z) + d_2(x_z) \leq d_1(x,y) + d_1(y,z) + d_2(x,y) + d_2(y,z) = d(x,y) + d(y,z) $
\end{enumerate}


\subsection*{c}
\textit{Hamming distance} on $\mathbf{\chi}$ is  a metric:
\begin{enumerate}
	\item non-negativity: obviously, the number of different positions is non-negative
	\item identity of indiscernibles: obviously, if $x$ and $y$ are the same for every position, $x$ must be the same as $y$
	\item symmetry: obviously since the relationship of different is symmetry, the number of positions $x$ is not equal to $y$ must be equal to 
	the number of positions $y$ is not equal to $x$ 
	\item triangle inequality: for any position $x$, if $x_i = z_i$, then $d(y_i,z_i) + d(x_i, y_i) \geq d(x_i, z_i) = 0$; if $x_i \neq z_i$, then
	$y_i$ differs from at either $x_i$ or $z_i$ or both, that is, $d(y_i,z_i) + d(x_i, y_i) \geq d(x_i, z_i) = 1$. Sum up every position, we must have
	$d(x,z) \leq d(x,y) + d(y,z)$
\end{enumerate}

\subsection*{d}
Squared Euclidean distance is not metric. 

Let $m = 1, x= 4, y= 2, z= 0$, then $d(x,z) = 16 \geq (y,z) + d(x,y) = 8$. Therefore, it doesn't satisfy 
triangle inequality. 


\subsection*{e}
\textit{Kullback-Leibler divergence} is not a metric. 

Let $p = (0.2, 0.8)^T, q = (0.4, 0.6)^T$,
then $d(p,q) = 0.2 \text{ log }0.5 + 0.5\text{ log } \frac{4}{3}$, $d(q,p) = 0.4 \text{ log } 2 + 0.6 \text{ log }\frac{3}{4}$,
obviously we have $d(p,q) \neq d(q,p)$. That is, Kullback-Leibler divergence doesn't satisfy 
\textit{symmetry} requirement of metric, and thus it is not a metric.
% \begin{enumerate}
% 	\item non-negativity: first we have the inequation:
% 	 \begin{equation}\label{equ:1e}
% 	 	\text{log}~x \leq x-1, \forall x > 0,  \text{the equality holds if and only if }
% 	 	 x = 1
% 	 \end{equation}
% 	Therefore,
% 	\begin{align}
% 	\label{equ:3e}
% 		\begin{split}
% 			d(p,q) &= \sum_{i}p_i \text{log}\frac{p_i}{q_i} \\
% 					&= -\sum_{i}p_i \text{log}\frac{q_i}{p_i} \\
% 					&\geq -\sum_{i}p_i (\frac{q_i}{p_i} - 1) \\
% 					&=-\sum_{i}(q_i - p_i) \\
% 					&=-\sum_{i} q_i + \sum_{i}p_i = 0
% 		\end{split}
% 	\end{align}
% 	\item identity of indiscernibles: If $p = q$, obviously we have $d(p,q) =0 $. For the equality $d(p,q) = 0$ in Equation~\ref{equ:3e} to hold, given Equation~\ref{equ:1e}, we need $\frac{q_i}{p_i} = 1, \forall i$, that is $p = q$.
% 	\item symmetry: 
% 	\item triangle inequality: 
% \end{enumerate}

\end{document}
